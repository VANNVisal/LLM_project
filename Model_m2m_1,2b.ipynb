{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model m2m 1.2b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure SentencePiece is installed\n",
    "try:\n",
    "    import sentencepiece\n",
    "except ImportError:\n",
    "    print(\"Installing SentencePiece...\")\n",
    "    subprocess.check_call([\"pip\", \"install\", \"sentencepiece\"])\n",
    "    import sentencepiece  # Retry importing after installation\n",
    "\n",
    "# Check if GPU is available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and tokenizer\n",
    "model_name = \"facebook/m2m100_1.2B\"\n",
    "print(\"Loading model and tokenizer...\")\n",
    "tokenizer = M2M100Tokenizer.from_pretrained(model_name)\n",
    "model = M2M100ForConditionalGeneration.from_pretrained(model_name).to(device)\n",
    "\n",
    "# Set source and target languages\n",
    "source_lang = \"km\"  # Correct Khmer language code\n",
    "target_lang = \"en\"  # Correct English language code\n",
    "tokenizer.src_lang = source_lang\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load dataset\n",
    "file_path = r\"dataset_khmer.csv\"\n",
    "if not os.path.exists(file_path):\n",
    "    raise FileNotFoundError(f\"Dataset file not found at {file_path}\")\n",
    "\n",
    "print(\"Loading dataset...\")\n",
    "dataset = load_dataset(\"csv\", data_files=file_path)[\"train\"]  # Load as a single dataset\n",
    "\n",
    "# Display column names for debugging\n",
    "print(\"Column names in dataset:\", dataset.column_names)\n",
    "\n",
    "# Dynamically rename columns if needed\n",
    "source_column = \"Action Sentence                                                                           \"\n",
    "target_column = \"Formatted Output\"\n",
    "\n",
    "if source_column not in dataset.column_names or target_column not in dataset.column_names:\n",
    "    raise ValueError(f\"Expected columns '{source_column}' and '{target_column}' not found in the dataset.\")\n",
    "\n",
    "# Rename columns for consistency\n",
    "dataset = dataset.rename_column(source_column, \"Source\")\n",
    "dataset = dataset.rename_column(target_column, \"Target\")\n",
    "\n",
    "# Remove rows with empty Source or Target\n",
    "print(\"Filtering dataset for valid rows...\")\n",
    "dataset = dataset.filter(lambda example: example[\"Source\"].strip() and example[\"Target\"].strip())\n",
    "\n",
    "# Split the dataset into train and test\n",
    "print(\"Splitting dataset...\")\n",
    "split_dataset = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "split_dataset = DatasetDict({\"train\": split_dataset[\"train\"], \"test\": split_dataset[\"test\"]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Tokenize the dataset\n",
    "def preprocess_function(examples):\n",
    "    print(\"Processing examples:\", examples[\"Source\"][:5])  # Debug input samples\n",
    "    print(\"Processing targets:\", examples[\"Target\"][:5])  # Debug output samples\n",
    "\n",
    "    inputs = examples[\"Source\"]\n",
    "    targets = examples[\"Target\"]\n",
    "\n",
    "    # Tokenize inputs and targets separately\n",
    "    model_inputs = tokenizer(\n",
    "        inputs,\n",
    "        max_length=128,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    labels = tokenizer(\n",
    "        targets,\n",
    "        max_length=128,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    # Add labels to the model inputs\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "print(\"Tokenizing dataset...\")\n",
    "tokenized_datasets = split_dataset.map(preprocess_function, batched=True, remove_columns=[\"Source\", \"Target\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,\n",
    "    save_total_limit=2,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    push_to_hub=False,\n",
    "    report_to=[\"none\"],\n",
    "    remove_unused_columns=False,  # Prevent removal of unused columns\n",
    ")\n",
    "\n",
    "# Define the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Evaluate the model\n",
    "print(\"Evaluating model...\")\n",
    "evaluation_results = trainer.evaluate()\n",
    "print(\"Evaluation Results:\", evaluation_results)\n",
    "\n",
    "# Save the fine-tuned model\n",
    "print(\"Saving the fine-tuned model...\")\n",
    "model.save_pretrained(\"./fine_tuned_m2m100\")\n",
    "tokenizer.save_pretrained(\"./fine_tuned_m2m100\")\n",
    "print(\"Model and tokenizer saved in ./fine_tuned_m2m100\")\n",
    "\n",
    "# Chatbot Testing Loop\n",
    "print(\"\\nStarting chatbot testing... Type 'exit' to quit.\")\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() == \"exit\":\n",
    "        print(\"Goodbye!\")\n",
    "        break\n",
    "    tokenizer.src_lang = source_lang\n",
    "    inputs = tokenizer(user_input, return_tensors=\"pt\", truncation=True, padding=\"max_length\").to(device)\n",
    "    outputs = model.generate(**inputs, forced_bos_token_id=tokenizer.get_lang_id(target_lang))\n",
    "    response = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "    print(f\"Bot: {response}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sacrebleu import corpus_bleu  # BLEU score for translation evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Fix column names issue\n",
    "def load_data(file_path):\n",
    "    data = pd.read_csv(file_path, encoding='utf-8')\n",
    "    data.columns = data.columns.str.strip()  # Remove extra spaces in column names\n",
    "    print(\"Dataset Columns:\", data.columns)\n",
    "    return data\n",
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Custom dataset class\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, input_texts, target_texts, tokenizer):\n",
    "        self.input_texts = input_texts.tolist()\n",
    "        self.target_texts = target_texts.tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        inputs = self.tokenizer(self.input_texts[idx], return_tensors='pt', padding='max_length', truncation=True, max_length=128)\n",
    "        targets = self.tokenizer(self.target_texts[idx], return_tensors='pt', padding='max_length', truncation=True, max_length=128)\n",
    "        return inputs['input_ids'].squeeze(), targets['input_ids'].squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sacrebleu import corpus_bleu\n",
    "\n",
    "# Fix column names issue\n",
    "def load_data(file_path):\n",
    "    data = pd.read_csv(file_path, encoding='utf-8')\n",
    "    data.columns = data.columns.str.strip()  # Remove extra spaces in column names\n",
    "    print(\"Dataset Columns:\", data.columns)\n",
    "    return data\n",
    "\n",
    "# Custom dataset class\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, input_texts, target_texts, tokenizer):\n",
    "        self.input_texts = input_texts.tolist()\n",
    "        self.target_texts = target_texts.tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        inputs = self.tokenizer(self.input_texts[idx], return_tensors='pt', padding='max_length', truncation=True, max_length=128)\n",
    "        targets = self.tokenizer(self.target_texts[idx], return_tensors='pt', padding='max_length', truncation=True, max_length=128)\n",
    "        return inputs['input_ids'].squeeze(0), targets['input_ids'].squeeze(0)\n",
    "\n",
    "# Collate function to handle variable-length sequences\n",
    "def collate_fn(batch):\n",
    "    inputs, targets = zip(*batch)\n",
    "    inputs = torch.nn.utils.rnn.pad_sequence(inputs, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    targets = torch.nn.utils.rnn.pad_sequence(targets, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    return inputs, targets\n",
    "\n",
    "# Load and preprocess dataset\n",
    "data = load_data('dataset_khmer.csv')\n",
    "if 'Action Sentence' not in data.columns or 'Formatted Output' not in data.columns:\n",
    "    raise ValueError(\"Dataset must contain 'Action Sentence' and 'Formatted Output' columns\")\n",
    "\n",
    "input_texts = data['Action Sentence']\n",
    "target_texts = data['Formatted Output']\n",
    "\n",
    "# Split dataset into training and validation sets\n",
    "train_inputs, val_inputs, train_targets, val_targets = train_test_split(input_texts, target_texts, test_size=0.1, random_state=42)\n",
    "\n",
    "# Initialize model and tokenizer\n",
    "model_name = \"facebook/m2m100_1.2B\"\n",
    "model = M2M100ForConditionalGeneration.from_pretrained(model_name)\n",
    "tokenizer = M2M100Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataset = TranslationDataset(train_inputs, train_targets, tokenizer)\n",
    "val_dataset = TranslationDataset(val_inputs, val_targets, tokenizer)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=4, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# Define training parameters\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Training loop with gradient clipping\n",
    "model.train()\n",
    "for epoch in range(3):  # Adjust the number of epochs as needed\n",
    "    epoch_loss = 0\n",
    "    for batch in train_dataloader:\n",
    "        inputs, targets = [b.to(device) for b in batch]\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids=inputs, labels=targets)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Prevent exploding gradients\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}, Loss: {epoch_loss/len(train_dataloader):.4f}\")\n",
    "\n",
    "# Save the trained model\n",
    "save_path = \"trained_model\"\n",
    "model.save_pretrained(save_path)\n",
    "tokenizer.save_pretrained(save_path)\n",
    "print(\"Model saved successfully!\")\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate(model, dataloader, tokenizer):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            inputs, targets = [b.to(device) for b in batch]\n",
    "            outputs = model.generate(input_ids=inputs)\n",
    "            preds = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "            labels = tokenizer.batch_decode(targets, skip_special_tokens=True)\n",
    "\n",
    "            all_preds.extend([p.lower() for p in preds])  # Lowercase for fair comparison\n",
    "            all_labels.extend([l.lower() for l in labels])\n",
    "    \n",
    "    # BLEU Score Evaluation\n",
    "    bleu_score = corpus_bleu(all_preds, [all_labels]).score\n",
    "    return bleu_score\n",
    "\n",
    "# Evaluate the model\n",
    "bleu = evaluate(model, val_dataloader, tokenizer)\n",
    "print(f\"BLEU Score: {bleu:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sacrebleu import corpus_bleu\n",
    "import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Fix column names issue\n",
    "def load_data(file_path):\n",
    "    data = pd.read_csv(file_path, encoding='utf-8')\n",
    "    data.columns = data.columns.str.strip()  # Remove extra spaces in column names\n",
    "    print(\"Dataset Columns:\", data.columns)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Custom dataset class\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, input_texts, target_texts, tokenizer):\n",
    "        self.input_texts = input_texts.tolist()\n",
    "        self.target_texts = target_texts.tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        inputs = self.tokenizer(self.input_texts[idx], return_tensors='pt', padding='max_length', truncation=True, max_length=128)\n",
    "        targets = self.tokenizer(self.target_texts[idx], return_tensors='pt', padding='max_length', truncation=True, max_length=128)\n",
    "        return inputs['input_ids'].squeeze(0), targets['input_ids'].squeeze(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Collate function to handle variable-length sequences\n",
    "def collate_fn(batch):\n",
    "    inputs, targets = zip(*batch)\n",
    "    inputs = torch.nn.utils.rnn.pad_sequence(inputs, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    targets = torch.nn.utils.rnn.pad_sequence(targets, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    return inputs, targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load and preprocess dataset\n",
    "data = load_data('dataset_khmer.csv')\n",
    "if 'Action Sentence' not in data.columns or 'Formatted Output' not in data.columns:\n",
    "    raise ValueError(\"Dataset must contain 'Action Sentence' and 'Formatted Output' columns\")\n",
    "\n",
    "input_texts = data['Action Sentence']\n",
    "target_texts = data['Formatted Output']\n",
    "\n",
    "# Split dataset into training and validation sets\n",
    "train_inputs, val_inputs, train_targets, val_targets = train_test_split(input_texts, target_texts, test_size=0.1, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize model and tokenizer\n",
    "model_name = \"facebook/m2m100_1.2B\"\n",
    "model = M2M100ForConditionalGeneration.from_pretrained(model_name)\n",
    "tokenizer = M2M100Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataset = TranslationDataset(train_inputs, train_targets, tokenizer)\n",
    "val_dataset = TranslationDataset(val_inputs, val_targets, tokenizer)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=4, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# Define training parameters\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "device = torch.device(\"cpu\")  # Force CPU execution\n",
    "model.to(device)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 Training:   1%|          | 1/113 [05:57<11:07:14, 357.45s/batch, loss=0.071]"
     ]
    }
   ],
   "source": [
    "\n",
    "# Training loop with gradient clipping\n",
    "model.train()\n",
    "for epoch in range(3):  # Adjust the number of epochs as needed\n",
    "    epoch_loss = 0\n",
    "    progress_bar = tqdm.tqdm(train_dataloader, desc=f\"Epoch {epoch+1} Training\", unit=\"batch\")\n",
    "    for batch in progress_bar:\n",
    "        inputs, targets = [b.to(device) for b in batch]\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids=inputs, labels=targets)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Prevent exploding gradients\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        progress_bar.set_postfix(loss=epoch_loss / len(train_dataloader))\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}, Loss: {epoch_loss/len(train_dataloader):.4f}\")\n",
    "\n",
    "# Save the trained model\n",
    "save_path = \"trained_model\"\n",
    "model.save_pretrained(save_path)\n",
    "tokenizer.save_pretrained(save_path)\n",
    "print(\"Model saved successfully!\")\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate(model, dataloader, tokenizer):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm.tqdm(dataloader, desc=\"Evaluating\", unit=\"batch\"):\n",
    "            inputs, targets = [b.to(device) for b in batch]\n",
    "            outputs = model.generate(input_ids=inputs)\n",
    "            preds = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "            labels = tokenizer.batch_decode(targets, skip_special_tokens=True)\n",
    "\n",
    "            all_preds.extend([p.lower() for p in preds])  # Lowercase for fair comparison\n",
    "            all_labels.extend([l.lower() for l in labels])\n",
    "    \n",
    "    # BLEU Score Evaluation\n",
    "    bleu_score = corpus_bleu(all_preds, [all_labels]).score\n",
    "    return bleu_score\n",
    "\n",
    "# Evaluate the model\n",
    "bleu = evaluate(model, val_dataloader, tokenizer)\n",
    "print(f\"BLEU Score: {bleu:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\BNC\\anaconda3\\envs\\airsim_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sacrebleu import corpus_bleu\n",
    "import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Fix column names issue\n",
    "def load_data(file_path):\n",
    "    data = pd.read_csv(file_path, encoding='utf-8')\n",
    "    data.columns = data.columns.str.strip()\n",
    "    print(\"Dataset Columns:\", data.columns)\n",
    "    return data\n",
    "\n",
    "# Custom dataset class\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, input_texts, target_texts, tokenizer):\n",
    "        self.input_texts = input_texts.tolist()\n",
    "        self.target_texts = target_texts.tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        inputs = self.tokenizer(self.input_texts[idx], return_tensors='pt', padding='longest', truncation=True, max_length=128)\n",
    "        targets = self.tokenizer(self.target_texts[idx], return_tensors='pt', padding='longest', truncation=True, max_length=128)\n",
    "        return inputs['input_ids'].squeeze(0), targets['input_ids'].squeeze(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Columns: Index(['Action Sentence', 'Formatted Output'], dtype='object')\n",
      "Downloading model... Ensure internet connection is active.\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "We couldn't connect to 'https://huggingface.co' to load this file, couldn't find it in the cached files and it looks like facebook/m2m100_418M is not the path to a directory containing a file named config.json.\nCheckout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mtimeout\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\BNC\\anaconda3\\envs\\airsim_env\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    535\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 536\u001b[1;33m             \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    537\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mBaseSSLError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mOSError\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\BNC\\anaconda3\\envs\\airsim_env\\lib\\site-packages\\urllib3\\connection.py\u001b[0m in \u001b[0;36mgetresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    506\u001b[0m         \u001b[1;31m# Get the response from http.client.HTTPConnection\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 507\u001b[1;33m         \u001b[0mhttplib_response\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    508\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\BNC\\anaconda3\\envs\\airsim_env\\lib\\http\\client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1347\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1348\u001b[1;33m                 \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1349\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\BNC\\anaconda3\\envs\\airsim_env\\lib\\http\\client.py\u001b[0m in \u001b[0;36mbegin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    315\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 316\u001b[1;33m             \u001b[0mversion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    317\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\BNC\\anaconda3\\envs\\airsim_env\\lib\\http\\client.py\u001b[0m in \u001b[0;36m_read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    276\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 277\u001b[1;33m         \u001b[0mline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"iso-8859-1\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    278\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\BNC\\anaconda3\\envs\\airsim_env\\lib\\socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    668\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 669\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    670\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\BNC\\anaconda3\\envs\\airsim_env\\lib\\ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1273\u001b[0m                   self.__class__)\n\u001b[1;32m-> 1274\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1275\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\BNC\\anaconda3\\envs\\airsim_env\\lib\\ssl.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1131\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1132\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1133\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mtimeout\u001b[0m: The read operation timed out",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mReadTimeoutError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\BNC\\anaconda3\\envs\\airsim_env\\lib\\site-packages\\requests\\adapters.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    666\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 667\u001b[1;33m             resp = conn.urlopen(\n\u001b[0m\u001b[0;32m    668\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\BNC\\anaconda3\\envs\\airsim_env\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    842\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 843\u001b[1;33m             retries = retries.increment(\n\u001b[0m\u001b[0;32m    844\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnew_e\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_pool\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_stacktrace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\BNC\\anaconda3\\envs\\airsim_env\\lib\\site-packages\\urllib3\\util\\retry.py\u001b[0m in \u001b[0;36mincrement\u001b[1;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[0;32m    473\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mread\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mFalse\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mmethod\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_method_retryable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 474\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mreraise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_stacktrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    475\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mread\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\BNC\\anaconda3\\envs\\airsim_env\\lib\\site-packages\\urllib3\\util\\util.py\u001b[0m in \u001b[0;36mreraise\u001b[1;34m(tp, value, tb)\u001b[0m\n\u001b[0;32m     38\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\BNC\\anaconda3\\envs\\airsim_env\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    788\u001b[0m             \u001b[1;31m# Make the request on the HTTPConnection object\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 789\u001b[1;33m             response = self._make_request(\n\u001b[0m\u001b[0;32m    790\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\BNC\\anaconda3\\envs\\airsim_env\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    537\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mBaseSSLError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mOSError\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 538\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_raise_timeout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mread_timeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    539\u001b[0m             \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\BNC\\anaconda3\\envs\\airsim_env\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_raise_timeout\u001b[1;34m(self, err, url, timeout_value)\u001b[0m\n\u001b[0;32m    368\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSocketTimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 369\u001b[1;33m             raise ReadTimeoutError(\n\u001b[0m\u001b[0;32m    370\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mf\"Read timed out. (read timeout={timeout_value})\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mReadTimeoutError\u001b[0m: HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mReadTimeout\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\BNC\\anaconda3\\envs\\airsim_env\\lib\\site-packages\\huggingface_hub\\file_download.py\u001b[0m in \u001b[0;36m_get_metadata_or_catch_error\u001b[1;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[0m\n\u001b[0;32m   1373\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1374\u001b[1;33m                 metadata = get_hf_file_metadata(\n\u001b[0m\u001b[0;32m   1375\u001b[0m                     \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproxies\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0metag_timeout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\BNC\\anaconda3\\envs\\airsim_env\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\BNC\\anaconda3\\envs\\airsim_env\\lib\\site-packages\\huggingface_hub\\file_download.py\u001b[0m in \u001b[0;36mget_hf_file_metadata\u001b[1;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001b[0m\n\u001b[0;32m   1293\u001b[0m     \u001b[1;31m# Retrieve metadata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1294\u001b[1;33m     r = _request_wrapper(\n\u001b[0m\u001b[0;32m   1295\u001b[0m         \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"HEAD\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\BNC\\anaconda3\\envs\\airsim_env\\lib\\site-packages\\huggingface_hub\\file_download.py\u001b[0m in \u001b[0;36m_request_wrapper\u001b[1;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[0;32m    277\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfollow_relative_redirects\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 278\u001b[1;33m         response = _request_wrapper(\n\u001b[0m\u001b[0;32m    279\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\BNC\\anaconda3\\envs\\airsim_env\\lib\\site-packages\\huggingface_hub\\file_download.py\u001b[0m in \u001b[0;36m_request_wrapper\u001b[1;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[0;32m    300\u001b[0m     \u001b[1;31m# Perform request and return if status_code is not in the retry list.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 301\u001b[1;33m     \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    302\u001b[0m     \u001b[0mhf_raise_for_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\BNC\\anaconda3\\envs\\airsim_env\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    588\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 589\u001b[1;33m         \u001b[0mresp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    590\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\BNC\\anaconda3\\envs\\airsim_env\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    702\u001b[0m         \u001b[1;31m# Send the request\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 703\u001b[1;33m         \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    704\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\BNC\\anaconda3\\envs\\airsim_env\\lib\\site-packages\\huggingface_hub\\utils\\_http.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, *args, **kwargs)\u001b[0m\n\u001b[0;32m     92\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRequestException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\BNC\\anaconda3\\envs\\airsim_env\\lib\\site-packages\\requests\\adapters.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    712\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mReadTimeoutError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 713\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mReadTimeout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    714\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_InvalidHeader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mReadTimeout\u001b[0m: (ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: a512e768-6f32-4048-9147-1e1a0f8c45c9)')",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mLocalEntryNotFoundError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\BNC\\anaconda3\\envs\\airsim_env\\lib\\site-packages\\transformers\\utils\\hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m    402\u001b[0m         \u001b[1;31m# Load from URL or cache if already cached\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 403\u001b[1;33m         resolved_file = hf_hub_download(\n\u001b[0m\u001b[0;32m    404\u001b[0m             \u001b[0mpath_or_repo_id\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\BNC\\anaconda3\\envs\\airsim_env\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\BNC\\anaconda3\\envs\\airsim_env\\lib\\site-packages\\huggingface_hub\\file_download.py\u001b[0m in \u001b[0;36mhf_hub_download\u001b[1;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[0;32m    859\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 860\u001b[1;33m         return _hf_hub_download_to_cache_dir(\n\u001b[0m\u001b[0;32m    861\u001b[0m             \u001b[1;31m# Destination\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\BNC\\anaconda3\\envs\\airsim_env\\lib\\site-packages\\huggingface_hub\\file_download.py\u001b[0m in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[1;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[0;32m    966\u001b[0m         \u001b[1;31m# Otherwise, raise appropriate error\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 967\u001b[1;33m         \u001b[0m_raise_on_head_call_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhead_call_error\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mforce_download\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocal_files_only\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    968\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\BNC\\anaconda3\\envs\\airsim_env\\lib\\site-packages\\huggingface_hub\\file_download.py\u001b[0m in \u001b[0;36m_raise_on_head_call_error\u001b[1;34m(head_call_error, force_download, local_files_only)\u001b[0m\n\u001b[0;32m   1484\u001b[0m         \u001b[1;31m# Otherwise: most likely a connection issue or Hub downtime => let's warn the user\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1485\u001b[1;33m         raise LocalEntryNotFoundError(\n\u001b[0m\u001b[0;32m   1486\u001b[0m             \u001b[1;34m\"An error happened while trying to locate the file on the Hub and we cannot find the requested files\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLocalEntryNotFoundError\u001b[0m: An error happened while trying to locate the file on the Hub and we cannot find the requested files in the local cache. Please check your connection and try again or make sure your Internet connection is on.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_20996\\1788939352.py\u001b[0m in \u001b[0;36m<cell line: 50>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Downloading model... Ensure internet connection is active.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mM2M100ForConditionalGeneration\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m     \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mM2M100Tokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\BNC\\anaconda3\\envs\\airsim_env\\lib\\site-packages\\transformers\\modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   3611\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mPretrainedConfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3612\u001b[0m             \u001b[0mconfig_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconfig\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mconfig\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3613\u001b[1;33m             config, model_kwargs = cls.config_class.from_pretrained(\n\u001b[0m\u001b[0;32m   3614\u001b[0m                 \u001b[0mconfig_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3615\u001b[0m                 \u001b[0mcache_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcache_dir\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\BNC\\anaconda3\\envs\\airsim_env\\lib\\site-packages\\transformers\\configuration_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, **kwargs)\u001b[0m\n\u001b[0;32m    543\u001b[0m         \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_token_in_kwargs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m         \u001b[0mconfig_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_config_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    546\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;34m\"model_type\"\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"model_type\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"model_type\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_type\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m             logger.warning(\n",
      "\u001b[1;32mc:\\Users\\BNC\\anaconda3\\envs\\airsim_env\\lib\\site-packages\\transformers\\configuration_utils.py\u001b[0m in \u001b[0;36mget_config_dict\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    572\u001b[0m         \u001b[0moriginal_kwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    573\u001b[0m         \u001b[1;31m# Get config dict associated with the base config file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 574\u001b[1;33m         \u001b[0mconfig_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_config_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    575\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mconfig_dict\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    576\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\BNC\\anaconda3\\envs\\airsim_env\\lib\\site-packages\\transformers\\configuration_utils.py\u001b[0m in \u001b[0;36m_get_config_dict\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    631\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    632\u001b[0m                 \u001b[1;31m# Load from local folder or from cache or download from model Hub and cache\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 633\u001b[1;33m                 resolved_config_file = cached_file(\n\u001b[0m\u001b[0;32m    634\u001b[0m                     \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    635\u001b[0m                     \u001b[0mconfiguration_file\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\BNC\\anaconda3\\envs\\airsim_env\\lib\\site-packages\\transformers\\utils\\hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m    444\u001b[0m         ):\n\u001b[0;32m    445\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresolved_file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 446\u001b[1;33m         raise EnvironmentError(\n\u001b[0m\u001b[0;32m    447\u001b[0m             \u001b[1;34mf\"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this file, couldn't find it in the\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    448\u001b[0m             \u001b[1;34mf\" cached files and it looks like {path_or_repo_id} is not the path to a directory containing a file named\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: We couldn't connect to 'https://huggingface.co' to load this file, couldn't find it in the cached files and it looks like facebook/m2m100_418M is not the path to a directory containing a file named config.json.\nCheckout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create DataLoaders\n",
    "train_dataset = TranslationDataset(train_inputs, train_targets, tokenizer)\n",
    "val_dataset = TranslationDataset(val_inputs, val_targets, tokenizer)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "# Define training parameters\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-5)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Training loop with mixed precision for speed\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "model.train()\n",
    "for epoch in range(3):\n",
    "    epoch_loss = 0\n",
    "    progress_bar = tqdm.tqdm(train_dataloader, desc=f\"Epoch {epoch+1} Training\", unit=\"batch\")\n",
    "    for batch in progress_bar:\n",
    "        inputs, targets = [b.to(device) for b in batch]\n",
    "        optimizer.zero_grad()\n",
    "        with torch.cuda.amp.autocast():\n",
    "            outputs = model(input_ids=inputs, labels=targets)\n",
    "            loss = outputs.loss\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        progress_bar.set_postfix(loss=epoch_loss / len(train_dataloader))\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}, Loss: {epoch_loss/len(train_dataloader):.4f}\")\n",
    "\n",
    "# Save the trained model\n",
    "save_path = \"trained_model\"\n",
    "model.save_pretrained(save_path)\n",
    "tokenizer.save_pretrained(save_path)\n",
    "print(\"Model saved successfully!\")\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate(model, dataloader, tokenizer):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm.tqdm(dataloader, desc=\"Evaluating\", unit=\"batch\"):\n",
    "            inputs, targets = [b.to(device) for b in batch]\n",
    "            outputs = model.generate(input_ids=inputs, max_length=128)\n",
    "            preds = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "            labels = tokenizer.batch_decode(targets, skip_special_tokens=True)\n",
    "\n",
    "            all_preds.extend([p.lower() for p in preds])\n",
    "            all_labels.extend([l.lower() for l in labels])\n",
    "    \n",
    "    bleu_score = corpus_bleu(all_preds, [all_labels]).score\n",
    "    return bleu_score\n",
    "\n",
    "# Evaluate the model\n",
    "bleu = evaluate(model, val_dataloader, tokenizer)\n",
    "print(f\"BLEU Score: {bleu:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\BNC\\anaconda3\\envs\\airsim_env\\lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\BNC\\.cache\\huggingface\\hub\\models--facebook--m2m100_418M. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\BNC\\anaconda3\\envs\\airsim_env\\lib\\site-packages\\transformers\\modeling_utils.py:2817: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 200, 'early_stopping': True, 'num_beams': 5}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./m2m100_418M\\\\tokenizer_config.json',\n",
       " './m2m100_418M\\\\special_tokens_map.json',\n",
       " 'm2m100_418M\\\\vocab.json',\n",
       " 'm2m100_418M\\\\sentencepiece.bpe.model',\n",
       " './m2m100_418M\\\\added_tokens.json')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer\n",
    "\n",
    "model_name = \"facebook/m2m100_418M\"\n",
    "model = M2M100ForConditionalGeneration.from_pretrained(model_name)\n",
    "tokenizer = M2M100Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Save to a local directory\n",
    "model.save_pretrained(\"./m2m100_418M\")\n",
    "tokenizer.save_pretrained(\"./m2m100_418M\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Columns: Index(['Action Sentence', 'Formatted Output'], dtype='object')\n",
      "Loading model from local storage...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 Training:   0%|          | 0/57 [00:00<?, ?batch/s]We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n",
      "Epoch 1 Training: 100%|██████████| 57/57 [12:18<00:00, 12.96s/batch, loss=0.584]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.5844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 Training: 100%|██████████| 57/57 [13:30<00:00, 14.22s/batch, loss=0.0985]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 0.0985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 Training: 100%|██████████| 57/57 [13:02<00:00, 13.73s/batch, loss=0.032]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: 0.0320\n",
      "Model saved successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 7/7 [04:13<00:00, 36.16s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score: 100.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sacrebleu import corpus_bleu\n",
    "import tqdm\n",
    "import os\n",
    "\n",
    "# Fix column names issue\n",
    "def load_data(file_path):\n",
    "    data = pd.read_csv(file_path, encoding='utf-8')\n",
    "    data.columns = data.columns.str.strip()\n",
    "    print(\"Dataset Columns:\", data.columns)\n",
    "    return data\n",
    "\n",
    "# Custom dataset class\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, input_texts, target_texts, tokenizer):\n",
    "        self.input_texts = input_texts.tolist()\n",
    "        self.target_texts = target_texts.tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        inputs = self.tokenizer(self.input_texts[idx], return_tensors='pt', padding='longest', truncation=True, max_length=128)\n",
    "        targets = self.tokenizer(self.target_texts[idx], return_tensors='pt', padding='longest', truncation=True, max_length=128)\n",
    "        return inputs['input_ids'].squeeze(0), targets['input_ids'].squeeze(0)\n",
    "\n",
    "# Collate function to handle variable-length sequences\n",
    "def collate_fn(batch):\n",
    "    input_ids, target_ids = zip(*batch)\n",
    "    input_ids = torch.nn.utils.rnn.pad_sequence(input_ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    target_ids = torch.nn.utils.rnn.pad_sequence(target_ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    return input_ids, target_ids\n",
    "\n",
    "# Load and preprocess dataset\n",
    "data = load_data('dataset_khmer.csv')\n",
    "if 'Action Sentence' not in data.columns or 'Formatted Output' not in data.columns:\n",
    "    raise ValueError(\"Dataset must contain 'Action Sentence' and 'Formatted Output' columns\")\n",
    "\n",
    "input_texts = data['Action Sentence']\n",
    "target_texts = data['Formatted Output']\n",
    "\n",
    "# Split dataset\n",
    "train_inputs, val_inputs, train_targets, val_targets = train_test_split(input_texts, target_texts, test_size=0.1, random_state=42)\n",
    "\n",
    "# Ensure offline mode if no internet\n",
    "os.environ[\"TRANSFORMERS_OFFLINE\"] = \"1\"\n",
    "\n",
    "# Load model from local storage if available\n",
    "model_path = \"./models/m2m100_418M\"\n",
    "if os.path.exists(model_path):\n",
    "    print(\"Loading model from local storage...\")\n",
    "    model = M2M100ForConditionalGeneration.from_pretrained(model_path)\n",
    "    tokenizer = M2M100Tokenizer.from_pretrained(model_path)\n",
    "else:\n",
    "    raise OSError(\"Model not found locally. Please download and place it in './models/m2m100_418M'\")\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataset = TranslationDataset(train_inputs, train_targets, tokenizer)\n",
    "val_dataset = TranslationDataset(val_inputs, val_targets, tokenizer)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=collate_fn)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=8, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# Define training parameters\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-5)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(3):\n",
    "    epoch_loss = 0\n",
    "    progress_bar = tqdm.tqdm(train_dataloader, desc=f\"Epoch {epoch+1} Training\", unit=\"batch\")\n",
    "    for batch in progress_bar:\n",
    "        inputs, targets = [b.to(device) for b in batch]\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(input_ids=inputs, labels=targets)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        progress_bar.set_postfix(loss=epoch_loss / len(train_dataloader))\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}, Loss: {epoch_loss/len(train_dataloader):.4f}\")\n",
    "\n",
    "# Save the trained model\n",
    "save_path = \"trained_model\"\n",
    "model.save_pretrained(save_path)\n",
    "tokenizer.save_pretrained(save_path)\n",
    "print(\"Model saved successfully!\")\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate(model, dataloader, tokenizer):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm.tqdm(dataloader, desc=\"Evaluating\", unit=\"batch\"):\n",
    "            inputs, targets = [b.to(device) for b in batch]\n",
    "            outputs = model.generate(input_ids=inputs, max_length=128)\n",
    "            preds = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "            labels = tokenizer.batch_decode(targets, skip_special_tokens=True)\n",
    "            all_preds.extend([p.lower() for p in preds])\n",
    "            all_labels.extend([l.lower() for l in labels])\n",
    "    \n",
    "    bleu_score = corpus_bleu(all_preds, [all_labels]).score\n",
    "    return bleu_score\n",
    "\n",
    "# Evaluate the model\n",
    "bleu = evaluate(model, val_dataloader, tokenizer)\n",
    "print(f\"BLEU Score: {bleu:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Columns: Index(['Action Sentence', 'Formatted Output'], dtype='object')\n",
      "Loading model from local storage...\n",
      "Chatbot is ready! Type 'exit' to quit.\n",
      "Bot: Қаланың тұрақты тұрғындарының тұрақты тұрғындарының тұрақты тұрғындарының тұрақты тұрғындарының тұрақты тұрғындарының тұрақты тұрғындарының тұрақты тұрғындарының саны 75 адамды құрайды.\n",
      "Bot: Қаланың тұрақты тұрғындарының тұрақты тұрғындарының тұрақты тұрғындарының тұрақты тұрғындарының тұрақты тұрғындарының тұрақты тұрғындарының тұрақты тұрғындарының саны 75 адамды құрайды.\n",
      "Bot: Қазіргі уақытта 7,68 метр\n",
      "Bot: Қаланың тұрақты тұрғындарының тұрақты тұрғындарының тұрақты тұрғындарының тұрақты тұрғындарының тұрақты тұрғындарының тұрақты тұрғындарының тұрақты тұрғындарының саны 75 адамды құрайды.\n",
      "Bot: Βοήθεια\n",
      "Bot: Қаланың тұрақты тұрғындарының тұрақты тұрғындарының тұрақты тұрғындарының тұрақты тұрғындарының тұрақты тұрғындарының тұрақты тұрғындарының тұрақты тұрғындарының саны 75 адамды құрайды.\n",
      "Bot: Қаланың тұрақты тұрғындарының тұрақты тұрғындарының тұрақты тұрғындарының тұрақты тұрғындарының тұрақты тұрғындарының тұрақты тұрғындарының тұрақты тұрғындарының саны 75 адамды құрайды.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sacrebleu import corpus_bleu\n",
    "import tqdm\n",
    "import os\n",
    "\n",
    "# Fix column names issue\n",
    "def load_data(file_path):\n",
    "    data = pd.read_csv(file_path, encoding='utf-8')\n",
    "    data.columns = data.columns.str.strip()\n",
    "    print(\"Dataset Columns:\", data.columns)\n",
    "    return data\n",
    "\n",
    "# Custom dataset class\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, input_texts, target_texts, tokenizer):\n",
    "        self.input_texts = input_texts.tolist()\n",
    "        self.target_texts = target_texts.tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        inputs = self.tokenizer(self.input_texts[idx], return_tensors='pt', padding='longest', truncation=True, max_length=128)\n",
    "        targets = self.tokenizer(self.target_texts[idx], return_tensors='pt', padding='longest', truncation=True, max_length=128)\n",
    "        return inputs['input_ids'].squeeze(0), targets['input_ids'].squeeze(0)\n",
    "\n",
    "# Collate function to handle variable-length sequences\n",
    "def collate_fn(batch):\n",
    "    input_ids, target_ids = zip(*batch)\n",
    "    input_ids = torch.nn.utils.rnn.pad_sequence(input_ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    target_ids = torch.nn.utils.rnn.pad_sequence(target_ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    return input_ids, target_ids\n",
    "\n",
    "# Load and preprocess dataset\n",
    "data = load_data('dataset_khmer.csv')\n",
    "if 'Action Sentence' not in data.columns or 'Formatted Output' not in data.columns:\n",
    "    raise ValueError(\"Dataset must contain 'Action Sentence' and 'Formatted Output' columns\")\n",
    "\n",
    "input_texts = data['Action Sentence']\n",
    "target_texts = data['Formatted Output']\n",
    "\n",
    "# Split dataset\n",
    "train_inputs, val_inputs, train_targets, val_targets = train_test_split(input_texts, target_texts, test_size=0.1, random_state=42)\n",
    "\n",
    "# Ensure offline mode if no internet\n",
    "os.environ[\"TRANSFORMERS_OFFLINE\"] = \"1\"\n",
    "\n",
    "# Load model from local storage if available\n",
    "model_path = \"./models/m2m100_418M\"\n",
    "if os.path.exists(model_path):\n",
    "    print(\"Loading model from local storage...\")\n",
    "    model = M2M100ForConditionalGeneration.from_pretrained(model_path)\n",
    "    tokenizer = M2M100Tokenizer.from_pretrained(model_path)\n",
    "else:\n",
    "    raise OSError(\"Model not found locally. Please download and place it in './models/m2m100_418M'\")\n",
    "\n",
    "# Chatbot function\n",
    "def chatbot():\n",
    "    print(\"Chatbot is ready! Type 'exit' to quit.\")\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        if user_input.lower() == 'exit':\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "        \n",
    "        inputs = tokenizer(user_input, return_tensors='pt', padding='longest', truncation=True, max_length=128).input_ids.to(device)\n",
    "        outputs = model.generate(input_ids=inputs, max_length=128)\n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        print(\"Bot:\", response)\n",
    "\n",
    "# Start chatbot\n",
    "# tokenizer does not need to be moved to device\n",
    "chatbot()ទៅមុខ 7.68 ម៉ែត្រ នៅល្បឿន 1.75 ម៉ែត្រ/វិនាទី, បន្ទាប់មក បង្វែស្ដាំ 74 ដឺក្រេ។\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "airsim_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
