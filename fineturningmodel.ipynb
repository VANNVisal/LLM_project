{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!pip install trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting trl\n",
      "  Downloading trl-0.11.4-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: torch>=1.4.0 in c:\\users\\bnc\\anaconda3\\envs\\airsim_env\\lib\\site-packages (from trl) (2.4.1)\n",
      "Requirement already satisfied: transformers>=4.40.0 in c:\\users\\bnc\\anaconda3\\envs\\airsim_env\\lib\\site-packages (from trl) (4.46.3)\n",
      "Requirement already satisfied: accelerate in c:\\users\\bnc\\anaconda3\\envs\\airsim_env\\lib\\site-packages (from trl) (1.0.1)\n",
      "Requirement already satisfied: datasets in c:\\users\\bnc\\anaconda3\\envs\\airsim_env\\lib\\site-packages (from trl) (3.1.0)\n",
      "Collecting tyro>=0.5.11 (from trl)\n",
      "  Downloading tyro-0.9.14-py3-none-any.whl.metadata (9.4 kB)\n",
      "Requirement already satisfied: numpy<2 in c:\\users\\bnc\\anaconda3\\envs\\airsim_env\\lib\\site-packages (from trl) (1.24.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\bnc\\anaconda3\\envs\\airsim_env\\lib\\site-packages (from torch>=1.4.0->trl) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\bnc\\anaconda3\\envs\\airsim_env\\lib\\site-packages (from torch>=1.4.0->trl) (4.12.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\bnc\\anaconda3\\envs\\airsim_env\\lib\\site-packages (from torch>=1.4.0->trl) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\bnc\\anaconda3\\envs\\airsim_env\\lib\\site-packages (from torch>=1.4.0->trl) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\bnc\\anaconda3\\envs\\airsim_env\\lib\\site-packages (from torch>=1.4.0->trl) (3.1.5)\n",
      "Requirement already satisfied: fsspec in c:\\users\\bnc\\anaconda3\\envs\\airsim_env\\lib\\site-packages (from torch>=1.4.0->trl) (2024.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\bnc\\anaconda3\\envs\\airsim_env\\lib\\site-packages (from transformers>=4.40.0->trl) (0.27.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\bnc\\anaconda3\\envs\\airsim_env\\lib\\site-packages (from transformers>=4.40.0->trl) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\bnc\\anaconda3\\envs\\airsim_env\\lib\\site-packages (from transformers>=4.40.0->trl) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\bnc\\anaconda3\\envs\\airsim_env\\lib\\site-packages (from transformers>=4.40.0->trl) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\bnc\\anaconda3\\envs\\airsim_env\\lib\\site-packages (from transformers>=4.40.0->trl) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\bnc\\anaconda3\\envs\\airsim_env\\lib\\site-packages (from transformers>=4.40.0->trl) (0.20.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\bnc\\anaconda3\\envs\\airsim_env\\lib\\site-packages (from transformers>=4.40.0->trl) (0.5.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\bnc\\anaconda3\\envs\\airsim_env\\lib\\site-packages (from transformers>=4.40.0->trl) (4.67.1)\n",
      "Requirement already satisfied: colorama>=0.4.0 in c:\\users\\bnc\\anaconda3\\envs\\airsim_env\\lib\\site-packages (from tyro>=0.5.11->trl) (0.4.6)\n",
      "Collecting docstring-parser>=0.15 (from tyro>=0.5.11->trl)\n",
      "  Downloading docstring_parser-0.16-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting eval-type-backport>=0.1.3 (from tyro>=0.5.11->trl)\n",
      "  Downloading eval_type_backport-0.2.2-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: rich>=11.1.0 in c:\\users\\bnc\\anaconda3\\envs\\airsim_env\\lib\\site-packages (from tyro>=0.5.11->trl) (13.9.4)\n",
      "Collecting shtab>=1.5.6 (from tyro>=0.5.11->trl)\n",
      "  Downloading shtab-1.7.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting typeguard>=4.0.0 (from tyro>=0.5.11->trl)\n",
      "  Downloading typeguard-4.4.0-py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: psutil in c:\\users\\bnc\\anaconda3\\envs\\airsim_env\\lib\\site-packages (from accelerate->trl) (5.9.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\bnc\\anaconda3\\envs\\airsim_env\\lib\\site-packages (from datasets->trl) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\bnc\\anaconda3\\envs\\airsim_env\\lib\\site-packages (from datasets->trl) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\bnc\\anaconda3\\envs\\airsim_env\\lib\\site-packages (from datasets->trl) (2.0.3)\n",
      "Requirement already satisfied: xxhash in c:\\users\\bnc\\anaconda3\\envs\\airsim_env\\lib\\site-packages (from datasets->trl) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\bnc\\anaconda3\\envs\\airsim_env\\lib\\site-packages (from datasets->trl) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\bnc\\anaconda3\\envs\\airsim_env\\lib\\site-packages (from datasets->trl) (3.10.11)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\bnc\\anaconda3\\envs\\airsim_env\\lib\\site-packages (from aiohttp->datasets->trl) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\bnc\\anaconda3\\envs\\airsim_env\\lib\\site-packages (from aiohttp->datasets->trl) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\bnc\\anaconda3\\envs\\airsim_env\\lib\\site-packages (from aiohttp->datasets->trl) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\bnc\\anaconda3\\envs\\airsim_env\\lib\\site-packages (from aiohttp->datasets->trl) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\bnc\\anaconda3\\envs\\airsim_env\\lib\\site-packages (from aiohttp->datasets->trl) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in c:\\users\\bnc\\anaconda3\\envs\\airsim_env\\lib\\site-packages (from aiohttp->datasets->trl) (1.15.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in c:\\users\\bnc\\anaconda3\\envs\\airsim_env\\lib\\site-packages (from aiohttp->datasets->trl) (5.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\bnc\\anaconda3\\envs\\airsim_env\\lib\\site-packages (from requests->transformers>=4.40.0->trl) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\bnc\\anaconda3\\envs\\airsim_env\\lib\\site-packages (from requests->transformers>=4.40.0->trl) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\bnc\\anaconda3\\envs\\airsim_env\\lib\\site-packages (from requests->transformers>=4.40.0->trl) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\bnc\\anaconda3\\envs\\airsim_env\\lib\\site-packages (from requests->transformers>=4.40.0->trl) (2024.12.14)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\bnc\\anaconda3\\envs\\airsim_env\\lib\\site-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\bnc\\anaconda3\\envs\\airsim_env\\lib\\site-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (2.18.0)\n",
      "Requirement already satisfied: importlib-metadata>=3.6 in c:\\users\\bnc\\anaconda3\\envs\\airsim_env\\lib\\site-packages (from typeguard>=4.0.0->tyro>=0.5.11->trl) (8.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\bnc\\anaconda3\\envs\\airsim_env\\lib\\site-packages (from jinja2->torch>=1.4.0->trl) (2.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\bnc\\anaconda3\\envs\\airsim_env\\lib\\site-packages (from pandas->datasets->trl) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\bnc\\anaconda3\\envs\\airsim_env\\lib\\site-packages (from pandas->datasets->trl) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\bnc\\anaconda3\\envs\\airsim_env\\lib\\site-packages (from pandas->datasets->trl) (2024.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\bnc\\anaconda3\\envs\\airsim_env\\lib\\site-packages (from sympy->torch>=1.4.0->trl) (1.3.0)\n",
      "Requirement already satisfied: zipp>=3.20 in c:\\users\\bnc\\anaconda3\\envs\\airsim_env\\lib\\site-packages (from importlib-metadata>=3.6->typeguard>=4.0.0->tyro>=0.5.11->trl) (3.21.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\bnc\\anaconda3\\envs\\airsim_env\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl) (0.1.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\bnc\\anaconda3\\envs\\airsim_env\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets->trl) (1.16.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\bnc\\anaconda3\\envs\\airsim_env\\lib\\site-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets->trl) (0.2.0)\n",
      "Downloading trl-0.11.4-py3-none-any.whl (316 kB)\n",
      "   ---------------------------------------- 0.0/316.6 kB ? eta -:--:--\n",
      "   ------------------- -------------------- 153.6/316.6 kB 3.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 316.6/316.6 kB 4.9 MB/s eta 0:00:00\n",
      "Downloading tyro-0.9.14-py3-none-any.whl (116 kB)\n",
      "   ---------------------------------------- 0.0/116.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 116.4/116.4 kB ? eta 0:00:00\n",
      "Downloading docstring_parser-0.16-py3-none-any.whl (36 kB)\n",
      "Downloading eval_type_backport-0.2.2-py3-none-any.whl (5.8 kB)\n",
      "Downloading shtab-1.7.1-py3-none-any.whl (14 kB)\n",
      "Downloading typeguard-4.4.0-py3-none-any.whl (35 kB)\n",
      "Installing collected packages: shtab, eval-type-backport, docstring-parser, typeguard, tyro, trl\n",
      "Successfully installed docstring-parser-0.16 eval-type-backport-0.2.2 shtab-1.7.1 trl-0.11.4 typeguard-4.4.0 tyro-0.9.14\n"
     ]
    }
   ],
   "source": [
    "!pip install trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\BNC\\anaconda3\\envs\\airsim_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mvannvisal1012\u001b[0m (\u001b[33mvannvisal1012-institute-of-tecnology-of-cambodia\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\BNC\\Documents\\ITC-Internship\\LLM\\LLM-Model\\visal\\wandb\\run-20250217_144121-y1ebdpm7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/vannvisal1012-institute-of-tecnology-of-cambodia/fine-tune-sea-llm/runs/y1ebdpm7' target=\"_blank\">drawn-dew-2</a></strong> to <a href='https://wandb.ai/vannvisal1012-institute-of-tecnology-of-cambodia/fine-tune-sea-llm' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/vannvisal1012-institute-of-tecnology-of-cambodia/fine-tune-sea-llm' target=\"_blank\">https://wandb.ai/vannvisal1012-institute-of-tecnology-of-cambodia/fine-tune-sea-llm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/vannvisal1012-institute-of-tecnology-of-cambodia/fine-tune-sea-llm/runs/y1ebdpm7' target=\"_blank\">https://wandb.ai/vannvisal1012-institute-of-tecnology-of-cambodia/fine-tune-sea-llm/runs/y1ebdpm7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "CUDA is required but not available for bitsandbytes. Please consider installing the multi-platform enabled version of bitsandbytes, which is currently a work in progress. Please check currently supported platforms and installation instructions at https://huggingface.co/docs/bitsandbytes/main/en/installation#multi-backend\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA is required but not available for bitsandbytes. Please consider installing the multi-platform enabled version of bitsandbytes, which is currently a work in progress. Please check currently supported platforms and installation instructions at https://huggingface.co/docs/bitsandbytes/main/en/installation#multi-backend",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_7372\\2997699478.py\u001b[0m in \u001b[0;36m<cell line: 36>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;31m# Load model with quantization\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[0mconfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAutoConfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m model = AutoModelForCausalLM.from_pretrained(\n\u001b[0m\u001b[0;32m     37\u001b[0m     \u001b[0mmodel_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquantization_config\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbnb_config\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice_map\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"auto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_cache\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m )\n",
      "\u001b[1;32mc:\\Users\\BNC\\anaconda3\\envs\\airsim_env\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    562\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    563\u001b[0m             \u001b[0mmodel_class\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_model_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 564\u001b[1;33m             return model_class.from_pretrained(\n\u001b[0m\u001b[0;32m    565\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mhub_kwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    566\u001b[0m             )\n",
      "\u001b[1;32mc:\\Users\\BNC\\anaconda3\\envs\\airsim_env\\lib\\site-packages\\transformers\\modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   3655\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3656\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhf_quantizer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3657\u001b[1;33m             hf_quantizer.validate_environment(\n\u001b[0m\u001b[0;32m   3658\u001b[0m                 \u001b[0mtorch_dtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch_dtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfrom_tf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfrom_tf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfrom_flax\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfrom_flax\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice_map\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdevice_map\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3659\u001b[0m             )\n",
      "\u001b[1;32mc:\\Users\\BNC\\anaconda3\\envs\\airsim_env\\lib\\site-packages\\transformers\\quantizers\\quantizer_bnb_4bit.py\u001b[0m in \u001b[0;36mvalidate_environment\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m         \u001b[0mbnb_multibackend_is_enabled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mis_bitsandbytes_multi_backend_available\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 82\u001b[1;33m         \u001b[0mvalidate_bnb_backend_availability\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraise_exception\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"from_tf\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"from_flax\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\BNC\\anaconda3\\envs\\airsim_env\\lib\\site-packages\\transformers\\integrations\\bitsandbytes.py\u001b[0m in \u001b[0;36mvalidate_bnb_backend_availability\u001b[1;34m(raise_exception)\u001b[0m\n\u001b[0;32m    556\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mis_bitsandbytes_multi_backend_available\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    557\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_validate_bnb_multi_backend_availability\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraise_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 558\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_validate_bnb_cuda_backend_availability\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraise_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\BNC\\anaconda3\\envs\\airsim_env\\lib\\site-packages\\transformers\\integrations\\bitsandbytes.py\u001b[0m in \u001b[0;36m_validate_bnb_cuda_backend_availability\u001b[1;34m(raise_exception)\u001b[0m\n\u001b[0;32m    534\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mraise_exception\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    535\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlog_msg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 536\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlog_msg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    537\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    538\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlog_msg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA is required but not available for bitsandbytes. Please consider installing the multi-platform enabled version of bitsandbytes, which is currently a work in progress. Please check currently supported platforms and installation instructions at https://huggingface.co/docs/bitsandbytes/main/en/installation#multi-backend"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments, TrainerCallback, AutoConfig\n",
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
    "from trl import SFTTrainer\n",
    "from datasets import load_dataset\n",
    "import wandb\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import evaluate\n",
    "\n",
    "# ---------------------------- Load Dataset ----------------------------\n",
    "# Define dataset paths\n",
    "train_path = r\"C:\\Users\\BNC\\Documents\\ITC-Internship\\LLM\\LLM-Model\\visal\\split_data\\train.jsonl\"  # Update with your actual path\n",
    "eval_path = r\"C:\\Users\\BNC\\Documents\\ITC-Internship\\LLM\\LLM-Model\\visal\\split_data\\valid.jsonl\"  # Update with your actual path\n",
    "\n",
    "# Load datasets\n",
    "train_dataset = load_dataset(\"json\", data_files=train_path, split=\"train\")\n",
    "eval_dataset = load_dataset(\"json\", data_files=eval_path, split=\"train\")\n",
    "\n",
    "# ---------------------------- Model Configuration ----------------------------\n",
    "model_id = \"SeaLLMs/SeaLLM-7B-v2.5\"  # SEA-LLM-7B-v1 Model\n",
    "\n",
    "wandb_config = {\"model\": model_id}\n",
    "wandb.init(project=\"fine-tune-sea-llm\", config=wandb_config)\n",
    "\n",
    "# Enable 4-bit Quantization for Efficient Training\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "# Load model with quantization\n",
    "config = AutoConfig.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, quantization_config=bnb_config, device_map=\"auto\", use_cache=False\n",
    ")\n",
    "\n",
    "# ---------------------------- Tokenizer Setup ----------------------------\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id, add_bos_token=True, add_eos_token=True\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# ---------------------------- LoRA Fine-Tuning Configuration ----------------------------\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    r=64,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# Prepare model for training with LoRA\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, peft_config)\n",
    "\n",
    "# ---------------------------- Dataset Preprocessing ----------------------------\n",
    "def create_prompt_universal(examples):\n",
    "    \"\"\"Formats dataset for fine-tuning SEA-LLM with Khmer input and English output.\"\"\"\n",
    "    output_text = []\n",
    "    for i in range(len(examples[\"khmer_input\"])):\n",
    "        input_text = examples[\"khmer_input\"][i]  # Khmer text\n",
    "        response = examples[\"robotic_command\"][i]  # English translation/action\n",
    "\n",
    "        chat_template = [\n",
    "            {\"role\": \"user\", \"content\": input_text},\n",
    "            {\"role\": \"assistant\", \"content\": response},\n",
    "        ]\n",
    "        prompt = tokenizer.apply_chat_template(chat_template, tokenize=False)\n",
    "        output_text.append(prompt)\n",
    "    return output_text\n",
    "\n",
    "# ---------------------------- Evaluation Metric ----------------------------\n",
    "def preprocess_logits_for_metrics(logits, labels):\n",
    "    \"\"\"Extracts predicted tokens from model output.\"\"\"\n",
    "    if isinstance(logits, tuple):\n",
    "        logits = logits[0]  # Extract logits\n",
    "    return logits.argmax(dim=-1)\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    \"\"\"Computes accuracy metric for evaluation.\"\"\"\n",
    "    preds, labels = eval_preds\n",
    "    labels = labels[:, 1:].reshape(-1)\n",
    "    preds = preds[:, :-1].reshape(-1)\n",
    "    return metric.compute(predictions=preds, references=labels)\n",
    "\n",
    "# ---------------------------- Training Arguments ----------------------------\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"sea_llm_finetuned\",\n",
    "    max_steps=2000,  # Increase for better learning\n",
    "    per_device_train_batch_size=4,  # Adjust based on available GPU memory\n",
    "    gradient_accumulation_steps=4,  # Helps with lower batch sizes\n",
    "    warmup_steps=100,\n",
    "    logging_steps=20,\n",
    "    save_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    learning_rate=2e-5,  # Adjust learning rate for stability\n",
    "    bf16=True,  # Use BF16 if on compatible GPUs\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    report_to=\"wandb\",\n",
    ")\n",
    "\n",
    "# ---------------------------- Callback for Loss Tracking ----------------------------\n",
    "class LossTrackerCallback(TrainerCallback):\n",
    "    \"\"\"Tracks loss values during training and plots learning curves.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.training_loss_values = []\n",
    "        self.eval_loss_values = []\n",
    "\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if \"loss\" in logs:\n",
    "            self.training_loss_values.append(logs[\"loss\"])\n",
    "        if \"eval_loss\" in logs:\n",
    "            self.eval_loss_values.append(logs[\"eval_loss\"])\n",
    "\n",
    "    def plot_learning_curve(self):\n",
    "        \"\"\"Plots training and evaluation loss curves.\"\"\"\n",
    "        plt.plot(self.training_loss_values, label=\"Training Loss\")\n",
    "        plt.plot(self.eval_loss_values, label=\"Evaluation Loss\")\n",
    "        plt.xlabel(\"Steps\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.title(\"Learning Curve\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "loss_tracker = LossTrackerCallback()\n",
    "\n",
    "# ---------------------------- Fine-Tuning ----------------------------\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    peft_config=peft_config,\n",
    "    max_seq_length=512,  # Increase if dataset has longer sequences\n",
    "    tokenizer=tokenizer,\n",
    "    formatting_func=create_prompt_universal,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    preprocess_logits_for_metrics=preprocess_logits_for_metrics,\n",
    "    callbacks=[loss_tracker],\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# ---------------------------- Save Model & Tokenizer ----------------------------\n",
    "trainer.model.save_pretrained(\"fine_tuned_sea_llm\")\n",
    "trainer.tokenizer.save_pretrained(\"fine_tuned_sea_llm\")\n",
    "trainer.model.config.save_pretrained(\"fine_tuned_sea_llm\")\n",
    "\n",
    "# ---------------------------- Plot Loss Curve ----------------------------\n",
    "loss_tracker.plot_learning_curve()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "airsim_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
